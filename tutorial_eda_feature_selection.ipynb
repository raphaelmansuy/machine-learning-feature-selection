{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf2ec09",
   "metadata": {},
   "source": [
    "# ðŸš€ SOTA Data Cleaning, Feature Selection, and EDA Tutorial\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/raphaelmansuy/machine-learning-feature-selection/blob/main/tutorial_eda_feature_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to this step-by-step tutorial on **State-of-the-Art (SOTA) Data Preparation** for Machine Learning.\n",
    "\n",
    "In this notebook, we will move beyond basic `pandas` and `matplotlib` workflows to use modern, high-performance tools that are standard in competitive data science (like Kaggle) and efficient production pipelines.\n",
    "\n",
    "### What you will learn:\n",
    "1.  **Why** and **How** to perform Exploratory Data Analysis (EDA) using **Sweetviz** (Automated EDA).\n",
    "2.  **Why** and **How** to clean data efficiently using **Pyjanitor**.\n",
    "3.  **Why** and **How** to select the most predictive features using **Polars** and **XGBoost**.\n",
    "\n",
    "### The \"SOTA\" Stack:\n",
    "*   **Polars**: A lightning-fast DataFrame library (faster and more memory-efficient than Pandas).\n",
    "*   **Sweetviz**: For generating beautiful, high-density EDA reports with one line of code.\n",
    "*   **Pyjanitor**: For clean, method-chaining data cleaning APIs.\n",
    "*   **XGBoost**: For powerful, gradient-boosted feature selection.\n",
    "\n",
    "Let's get started! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60137ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ“¦ Setup and Installation\n",
    "# We install the necessary libraries.\n",
    "# - polars: For fast data manipulation.\n",
    "# - sweetviz: For automated EDA.\n",
    "# - pyjanitor: For data cleaning.\n",
    "# - xgboost: For feature selection and modeling.\n",
    "\n",
    "# NOTE: Sweetviz is not yet compatible with NumPy 2.x in some environments\n",
    "# (you may see: AttributeError: module 'numpy' has no attribute 'VisibleDeprecationWarning').\n",
    "# To avoid this in Colab, install a NumPy 1.x release first, then the other packages.\n",
    "!pip install \"numpy<2.0\" -q\n",
    "!pip install polars sweetviz pyjanitor xgboost scikit-learn -q\n",
    "\n",
    "print(\"âœ… Libraries installed successfully (NumPy pinned to <2.0).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a41f90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ“š Import Libraries\n",
    "import polars as pl\n",
    "import pandas as pd # Sweetviz and Pyjanitor still rely on Pandas for some ops\n",
    "import sweetviz as sv\n",
    "import janitor # Registers pyjanitor methods on Pandas DataFrames\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure Polars to display more columns\n",
    "pl.Config.set_tbl_cols(20)\n",
    "\n",
    "print(\"âœ… Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedf230c",
   "metadata": {},
   "source": [
    "## 1. Data Loading ðŸ“¥\n",
    "\n",
    "We will use the **California Housing Dataset**. It's a classic dataset for regression tasks (predicting house prices).\n",
    "\n",
    "**Why Polars?**\n",
    "While this dataset is small, in real-world scenarios with millions of rows, `pandas` can be slow and memory-hungry. `polars` is written in Rust and is designed to be parallel and lazy-evaluated, making it much faster.\n",
    "\n",
    "*Note: Sweetviz and Pyjanitor currently work best with Pandas, so we will switch between Polars (for heavy lifting) and Pandas (for specific tools) as needed. This is a common pattern.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf35d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from sklearn\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df_pandas = data.frame\n",
    "\n",
    "# Add some noise/bad columns to demonstrate cleaning later\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "df_pandas['Bad Column Name'] = np.random.rand(len(df_pandas)) # Space in name\n",
    "df_pandas['useless_constant'] = 1 # Constant value\n",
    "df_pandas['duplicated_row_id'] = np.random.randint(0, 100, len(df_pandas)) # Random noise\n",
    "\n",
    "# Convert to Polars for inspection\n",
    "df_polars = pl.from_pandas(df_pandas)\n",
    "\n",
    "print(f\"Dataset Shape: {df_polars.shape}\")\n",
    "print(\"First 5 rows:\")\n",
    "print(df_polars.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3203cf7",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA) ðŸ”\n",
    "\n",
    "**Why EDA?**\n",
    "Before training any model, you **must** understand your data.\n",
    "- Are there missing values?\n",
    "- Are there outliers?\n",
    "- How are features distributed?\n",
    "- Is the target variable balanced?\n",
    "\n",
    "**The Old Way:**\n",
    "Manually plotting histograms and scatter plots for every column. This is slow and error-prone.\n",
    "\n",
    "**The SOTA Way (Sweetviz):**\n",
    "`sweetviz` generates a comprehensive HTML report with a single line of code. It compares training vs. testing data (if provided) and visualizes target correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffdddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the dataset\n",
    "# We use the Pandas DataFrame here as Sweetviz expects it\n",
    "report = sv.analyze(df_pandas)\n",
    "\n",
    "# Display the report\n",
    "# In Colab, this will render an interactive HTML report inside the notebook\n",
    "report.show_notebook(w=\"100%\", h=\"full\")\n",
    "\n",
    "# If you were running locally, you could use:\n",
    "# report.show_html(\"eda_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed7376",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning ðŸ§¹\n",
    "\n",
    "**Why Clean?**\n",
    "\"Garbage In, Garbage Out\". Models cannot learn effectively from messy data.\n",
    "Common issues:\n",
    "- Inconsistent column names (e.g., \"Bad Column Name\" vs \"bad_column_name\").\n",
    "- Constant columns (provide no information).\n",
    "- Duplicates.\n",
    "\n",
    "**The SOTA Way (Pyjanitor):**\n",
    "`pyjanitor` extends Pandas with a clean, method-chaining API. It makes cleaning code readable and reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f9ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data using method chaining\n",
    "# 1. clean_names(): Converts \"Bad Column Name\" -> \"bad_column_name\" (snake_case)\n",
    "# 2. remove_empty(): Removes columns/rows that are entirely empty\n",
    "# 3. drop_constant_columns(): Removes columns with a single unique value\n",
    "\n",
    "df_clean_pandas = (\n",
    "    df_pandas\n",
    "    .clean_names()\n",
    "    .remove_empty()\n",
    "    # We manually drop the constant column we added for demonstration if pyjanitor doesn't catch it with default settings\n",
    "    # But let's see if we can use a janitor method or pandas method\n",
    ")\n",
    "\n",
    "# Let's drop the constant column explicitly to be sure, or use variance threshold later.\n",
    "# For now, let's just clean names and convert back to Polars for speed.\n",
    "df_clean_pandas = df_clean_pandas.drop(columns=['useless_constant'])\n",
    "\n",
    "print(\"Columns before cleaning:\", df_pandas.columns.tolist())\n",
    "print(\"Columns after cleaning:\", df_clean_pandas.columns.tolist())\n",
    "\n",
    "# Convert back to Polars\n",
    "df_clean = pl.from_pandas(df_clean_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a204e613",
   "metadata": {},
   "source": [
    "## 4. Feature Selection ðŸŽ¯\n",
    "\n",
    "**Why Select Features?**\n",
    "- **Curse of Dimensionality**: Too many features can lead to overfitting.\n",
    "- **Performance**: Fewer features = faster training and inference.\n",
    "- **Interpretability**: Easier to explain the model.\n",
    "\n",
    "We will use a two-step approach:\n",
    "1.  **Filter Method**: Remove highly correlated features (Multicollinearity).\n",
    "2.  **Embedded Method**: Use XGBoost to find the most important features.\n",
    "\n",
    "### 4.1 Filter Method: Correlation Analysis\n",
    "Highly correlated features (e.g., > 0.95) carry redundant information. We can drop one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8706bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix using Polars\n",
    "# Polars is very fast at this\n",
    "corr_matrix = df_clean.corr()\n",
    "\n",
    "# Visualize with Seaborn (easier to see)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Strategy: Find pairs with correlation > 0.9 and drop one\n",
    "# For this dataset, features might not be super correlated, but let's check.\n",
    "# 'AveRooms' and 'AveBedrms' often have high correlation.\n",
    "\n",
    "def get_correlated_features(df, threshold=0.85):\n",
    "    corr_matrix = df.corr()\n",
    "    to_drop = set()\n",
    "    columns = df.columns\n",
    "    \n",
    "    for i in range(len(columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix[i, j]) > threshold:\n",
    "                colname = columns[i]\n",
    "                to_drop.add(colname)\n",
    "    return list(to_drop)\n",
    "\n",
    "features_to_drop = get_correlated_features(df_clean)\n",
    "print(f\"Features to drop (High Correlation > 0.85): {features_to_drop}\")\n",
    "\n",
    "# Drop them\n",
    "df_selected_1 = df_clean.drop(features_to_drop)\n",
    "print(f\"Remaining columns: {df_selected_1.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd60c9b",
   "metadata": {},
   "source": [
    "### 4.2 Embedded Method: XGBoost Feature Importance\n",
    "\n",
    "After filtering, we use a powerful model like XGBoost to determine which features actually contribute to predicting the target.\n",
    "\n",
    "**Why XGBoost?**\n",
    "It naturally handles non-linear relationships and interactions between features. It provides a `feature_importances_` attribute that tells us how often a feature was used to make key decisions in the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b593d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "target_col = \"MedHouseVal\" # The target in California Housing\n",
    "# Note: clean_names might have changed it to 'med_house_val' if we ran it on the target too.\n",
    "# Let's check columns again.\n",
    "print(df_selected_1.columns)\n",
    "\n",
    "# Assuming standard names from sklearn if clean_names didn't change target (it usually lowercases everything)\n",
    "# Let's find the target column name dynamically\n",
    "target_col = [c for c in df_selected_1.columns if \"medhouseval\" in c.lower()][0]\n",
    "print(f\"Target column identified as: {target_col}\")\n",
    "\n",
    "X = df_selected_1.drop(target_col).to_pandas()\n",
    "y = df_selected_1[target_col].to_pandas()\n",
    "\n",
    "# Train a quick XGBoost model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Plot feature importance\n",
    "xgb.plot_importance(model)\n",
    "plt.title(\"XGBoost Feature Importance\")\n",
    "plt.show()\n",
    "\n",
    "# Select top features (e.g., top 5)\n",
    "# In a real pipeline, you might use a threshold or SelectFromModel\n",
    "sorted_idx = model.feature_importances_.argsort()[::-1]\n",
    "top_features = X.columns[sorted_idx][:5]\n",
    "\n",
    "print(f\"Top 5 Features selected by XGBoost: {top_features.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a671e1",
   "metadata": {},
   "source": [
    "## 5. Conclusion ðŸŽ“\n",
    "\n",
    "We have successfully:\n",
    "1.  **Loaded** data efficiently.\n",
    "2.  **Visualized** it automatically with `sweetviz`.\n",
    "3.  **Cleaned** it with `pyjanitor`.\n",
    "4.  **Selected** the best features using Correlation and XGBoost.\n",
    "\n",
    "**Next Steps:**\n",
    "- Try this workflow on your own dataset!\n",
    "- Experiment with `polars` for larger datasets (1GB+).\n",
    "- Tune the XGBoost hyperparameters for better selection.\n",
    "\n",
    "Happy Coding! ðŸ’»"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
